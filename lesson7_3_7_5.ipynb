{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Vb-gwzzzpoxo",
        "6El-Jpe7pngt",
        "NeKD070gEjI0"
      ],
      "authorship_tag": "ABX9TyMt1nCbk594foqnvJ0uORxX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xtbtds/ml-zoomcamp/blob/main/lesson7_3_7_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.3 Deploying Your Prediction Service"
      ],
      "metadata": {
        "id": "Vb-gwzzzpoxo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `bentoml models list`\n",
        "- `bentoml models get <tag>`\n",
        "- `vim bentofile.yaml`$ - $ for building\n",
        "- `bentoml build`\n",
        "- `bentoml containerize <tag>` $ - $ build docker image\n",
        "\n",
        "We can look into this directory by locating `cd ~/bentoml/bentos/credit_risk_classifier/kdelkqsqms4i2b6d/` and see the file structure.\n"
      ],
      "metadata": {
        "id": "ZN63km8So1CQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.4 Sending, Receiving and Validating Data\n"
      ],
      "metadata": {
        "id": "6El-Jpe7pngt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What if we will send another fields or unknown fields of data? It won't fail by the way, but we *WANT* it to fail in such cases. For such purposes there is a library $ - $ **pydantic**.\n",
        "- `pip install pydantic`\n",
        "- `from pydantic import BaseModel`\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q4HBIIw3rMkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create pydantic base class to create data schema for validation\n",
        "class CreditApplication(BaseModel):\n",
        "    seniority: int\n",
        "    home: str\n",
        "    time: int\n",
        "    age: int\n",
        "    marital: str\n",
        "    records: str\n",
        "    job: str\n",
        "    expenses: int\n",
        "    income: float\n",
        "    assets: float\n",
        "    debt: float\n",
        "    amount: int\n",
        "    price: int"
      ],
      "metadata": {
        "id": "nY3E3jfJEL2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The BaseModel will ensure that we are always recieving this 13 features for the model prediction."
      ],
      "metadata": {
        "id": "PCnuCdo9EMrC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass pydantic class in the application\n",
        "@svc.api(input=JSON(pydantic_model=CreditApplication), output=JSON()) # decorate endpoint as in json format for input and output\n",
        "def classify(credit_application):\n",
        "    # transform pydantic class to dict to extract key-value pairs \n",
        "    application = credit_application.dict()\n",
        "    # transform data from client using dictvectorizer\n",
        "    vector = dv.transform(application)\n",
        "    # make predictions using 'runner.predict.run(input)' instead of 'model.predict'\n",
        "    prediction = model_runner.predict.run(vector) "
      ],
      "metadata": {
        "id": "rNRU_MxUEcSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Along the `JSON()`, BentoML uses various other descriptors in the input and output specification of the service api, for example, NumpyNdarray(), PandasDataFrame(), Text(), and many more."
      ],
      "metadata": {
        "id": "Z_SC78MaEgXH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.5 High-Performance Serving"
      ],
      "metadata": {
        "id": "NeKD070gEjI0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Locust** - library for load testing\n",
        "- `pip install locust`\n"
      ],
      "metadata": {
        "id": "WlUB4h4mEoDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# locustfile.py\n",
        "import numpy as np\n",
        "from locust import task\n",
        "from locust import between\n",
        "from locust import HttpUser\n",
        "\n",
        "\n",
        "# Sample data to send\n",
        "sample = {\"seniority\": 3,\n",
        " \"home\": \"owner\",\n",
        " \"time\": 36,\n",
        " \"age\": 26,\n",
        " \"marital\": \"single\",\n",
        " \"records\": \"no\",\n",
        " \"job\": \"freelance\",\n",
        " \"expenses\": 35,\n",
        " \"income\": 0.0,\n",
        " \"assets\": 60000.0,\n",
        " \"debt\": 3000.0,\n",
        " \"amount\": 800,\n",
        " \"price\": 1000\n",
        " }\n",
        "\n",
        "# Inherit HttpUser object from locust\n",
        "class CreditRiskTestUser(HttpUser):\n",
        "    \"\"\"\n",
        "    Usage:\n",
        "        Start locust load testing client with:\n",
        "            locust -H http://localhost:3000, in case if all requests failed then load client with:\n",
        "            locust -H http://localhost:3000 -f locustfile.py\n",
        "\n",
        "        Open browser at http://0.0.0.0:8089, adjust desired number of users and spawn\n",
        "        rate for the load test from the Web UI and start swarming.\n",
        "    \"\"\"\n",
        "\n",
        "    # create mathod with task decorator to send request\n",
        "    @task\n",
        "    def classify(self):\n",
        "        self.client.post(\"/classify\", json=sample) # post request in json format with the endpoint 'classify'\n",
        "\n",
        "    wait_time = between(0.01, 2) # set random wait time between 0.01-2 secs"
      ],
      "metadata": {
        "id": "46_pOSkXD6nP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.  **async** optimization\n",
        "Process the requests in parallel and the model will make predictions simultaneously."
      ],
      "metadata": {
        "id": "_-KOK3ToECHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define an endpoint on the BentoML service\n",
        "# pass pydantic class application\n",
        "@svc.api(input=JSON(pydantic_model=CreditApplication), output=JSON()) # decorate endpoint as in json format for input and output\n",
        "async def classify(credit_application): # parallelized requests at endpoint level (async)\n",
        "    # transform pydantic class to dict to extract key-value pairs \n",
        "    application = credit_application.dict()\n",
        "    # transform data from client using dictvectorizer\n",
        "    vector = dv.transform(application)\n",
        "    # make predictions using 'runner.predict.run(input)' instead of 'model.predict'\n",
        "    prediction = await model_runner.predict.async_run(vector) # bentoml inference level parallelization (async_run)"
      ],
      "metadata": {
        "id": "bq6YdIUuEmRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. **micro-batching** optimization\n",
        "Combine the data coming from multiple users and combine them into one array, and then this array will be batched into smaller batches when the model prediction is called."
      ],
      "metadata": {
        "id": "eJCGdsemE1pL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model batchable settings for production efficiency\n",
        "bentoml.xgboost.save_model('credit_risk_model',\n",
        "                            model,\n",
        "                            custom_objects={'DictVectorizer': dv},\n",
        "                           signatures={  # model signatures for runner inference\n",
        "                               'predict': { \n",
        "                                   'batchable': True, \n",
        "                                   'batch_dim': 0 # '0' means bentoml will concatenate request arrays by first dimension\n",
        "                               }\n",
        "                           })"
      ],
      "metadata": {
        "id": "TnjTnLijFKFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `bentoml serve --production` $ - $ make the batchable model in serving, `--production` flag will enable more than one process for our web workers\n",
        "\n",
        "`bentoconfiguration.yaml`:\n",
        "```\n",
        "# Config file controls the attributes of the runner\n",
        "runners:\n",
        "  batching:\n",
        "    enabled: true\n",
        "    max_batch_size: 100\n",
        "    max_latency_ms: 500\n",
        "```\n"
      ],
      "metadata": {
        "id": "sFQ8Uz4kFM7c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "InPKHqlaFz-z"
      }
    }
  ]
}