{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN4u88Y+vbzPSLd3kd2A7yk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xtbtds/ml-zoomcamp/blob/main/lesson4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.2 Accuracy and Dummy Model\n",
        "\n",
        "**Accuracy** measures the fraction of correct predictions. Specifically, it is the number of correct predictions divided by the total number of predictions.\n",
        "```\n",
        "accuracy_score(y_val, y_pred >= 0.5)\n",
        "```\n",
        "\n",
        "We can change the decision **threshold**, it should not be always 0.5. But, in this particular problem, the best decision cutoff, associated with the hightest accuracy (80%), was indeed 0.5.\n",
        "```\n",
        "thresholds = np.linspace(0, 1, 21)\n",
        "\n",
        "scores = []\n",
        "\n",
        "for t in thresholds:\n",
        "    score = accuracy_score(y_val, y_pred >= t)\n",
        "    print('%.2f %.3f' % (t, score))\n",
        "    scores.append(score)\n",
        "```\n",
        "\n",
        "Note that if we build a **dummy model** in which the decision cutoff is 1, so the algorithm *predicts that no clients will churn*, the accuracy would be 73%. Thus, we can see that the improvement of the original model with respect to the dummy model is not as high as we would expect.\n",
        "```\n",
        "from collections import Counter\n",
        "Counter(y_pred >= 1.0)\n",
        "1 - y_val.mean()\n",
        "```\n",
        "\n",
        "Therefore, in this problem accuracy can not tell us how good is the model because the dataset is **unbalanced**, which means that there are more instances from one category than the other. This is also known as **class imbalance**."
      ],
      "metadata": {
        "id": "2fXdyfCS8D48"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.3 Confusion table\n",
        "\n",
        "Working with unbalanced classes.\n",
        "\n",
        "Confusion table is a way of measuring different types of errors and correct decisions that binary classifiers can make. Considering this information, it is possible to evaluate the quality of the model by different strategies.\n",
        "\n",
        "When comes to a prediction of an LR model, each falls into one of four different categories:\n",
        "\n",
        "- Prediction is that the customer WILL churn. This is known as the Positive class\n",
        "\n",
        "  - And Customer actually churned - Known as a **True Positive (TP)**\n",
        "\n",
        "    ```\n",
        "    tp = (predict_positive & actual_positive).sum()\n",
        "    ```\n",
        "\n",
        "  - But Customer actually did not churn - Knwon as a **False Positive (FP)**\n",
        "    ```\n",
        "    fp = (predict_positive & actual_negative).sum()\n",
        "    ```\n",
        "\n",
        "- Prediction is that the customer WILL NOT churn' - This is known as the Negative class\n",
        "\n",
        "  - Customer did not churn - **True Negative (TN)**\n",
        "\n",
        "    ```\n",
        "    tn = (predict_negative & actual_negative).sum()\n",
        "    ```\n",
        "\n",
        "  - Customer churned - **False Negative (FN)**\n",
        "    ```\n",
        "    fn = (predict_negative & actual_positive).sum()\n",
        "    ```\n"
      ],
      "metadata": {
        "id": "4bjxQsY2A6FP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.4 Precision and Recall\n",
        "\n",
        "**Precision** tell us the fraction of positive predictions that are correct. It takes into account only the positive class (TP and FP - second column of the confusion matrix), as is stated in the following formula:\n",
        "\n",
        " $$P = \\frac{TP}{TP+FP}$$\n",
        "\n",
        "**Recall** measures the fraction of correctly identified postive instances. It considers parts of the postive and negative classes (TP and FN - second row of confusion table). The formula of this metric is presented below:\n",
        "\n",
        " $$R = \\frac{TP}{TP+FN}$$\n",
        "\n",
        "In this problem, the precision and recall values were 67% and 54% respectively. So, these measures reflect some errors of our model that accuracy did not notice due to the class imbalance."
      ],
      "metadata": {
        "id": "NuSM-Ub8xy26"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.5 ROC Curves\n",
        "\n",
        "**ROC** stands for Receiver Operating Characteristic, and this idea was applied during the Second World War for evaluating the strenght of radio detectors. This measure considers **False Positive Rate (FPR)** and **True Postive Rate (TPR)**, which are derived from the values of the confusion matrix.\n",
        "\n",
        "**FPR** is the fraction of false positives (FP) divided by the total number of negatives (FP and TN - the first row of confusion matrix), and we want to **minimize** it.\n",
        "$$FPR = \\frac{FP}{FP+TN}$$\n",
        "\n",
        "\n",
        "**TPR or Recall** is the fraction of true positives (TP) divided by the total number of positives (FN and TP - second row of confusion table), and we want to **maximize** this metric.\n",
        "$$TPR = \\frac{TP}{TP + FN}$$\n",
        "\n",
        "\n",
        "ROC curves consider Recall and FPR under all the possible thresholds. If the threshold is 0 or 1, the TPR and Recall scores are the opposite of the threshold (1 and 0 respectively), but they have different meanings, as we explained before.\n",
        "\n",
        "We need to compare the ROC curves against a point of reference to evaluate its performance, so the corresponding curves of **random** and **ideal models**are required. It is possible to plot the ROC curves with FPR and Recall scores vs thresholds, or FPR vs Recall."
      ],
      "metadata": {
        "id": "WHbphoTa0-BV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.6 ROC AUC\n",
        "\n",
        "The ***A***rea ***U***nder the ROC ***C***urves can tell us how good is our model with a single value. The AUC ROC of a random model is 0.5, while for an ideal one is 1.\n",
        "\n",
        "In ther words, AUC can be interpreted as the probability that a randomly selected positive example has a greater score than a randomly selected negative example."
      ],
      "metadata": {
        "id": "njcGg23JIZ4x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.7 Cross-Validation\n",
        "\n",
        "The full training dataset is divided into **k partitions**, we train the model in k-1 partiions of this dataset and evaluate it on the remaining subset. Then, we end up evaluating the model in all the k folds, and we calculate the average evaluation metric for all the folds.\n",
        "```\n",
        "kfold = KFold(n_splits=n_splits, shuffle=True, random_state=1)\n",
        "```\n",
        "This method is applied in the **parameter tuning** step, which is the process of selecting the best parameter.\n",
        "\n",
        "\n",
        "In general, if the dataset is large, we should use the hold-out validation dataset strategy. In the other hand, if the dataset is small or we want to know the standard deviation of the model across different folds, we can use the cross-validation approach."
      ],
      "metadata": {
        "id": "kPg2SeAqP5vw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notes\n",
        "\n",
        "**F1 score:**\n",
        "$$F_1 = \\frac{2PR}{P+R}$$\n",
        "\n",
        "**PR curve**\n",
        "- precision/recall curve\n",
        "- also useful metric"
      ],
      "metadata": {
        "id": "2vKGunoW7Fp_"
      }
    }
  ]
}