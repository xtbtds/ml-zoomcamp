{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNokY++y/y90s/ow5CRM4Cz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xtbtds/ml-zoomcamp/blob/main/lesson6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.3 Decision trees\n",
        "\n",
        "**Decision trees** make predictions based on the bunch of if/else statements by splitting a node into two or more sub-nodes.\n",
        "\n",
        "The decision tree is also prone to overfitting. One of the reason why this algorithm often overfits because of its **depth**. It tends to **memorize** all the patterns in the train data but struggle to performs well on the unseen data (validation or test set). To overcome with it, we reduce depth size.\n",
        "\n",
        "```\n",
        "dt = DecisionTreeClassifier(max_depth=3)\n",
        "```\n",
        "To print the tree:\n",
        "```\n",
        "from sklearn.tree import export_text\n",
        "print(export_text(dt, feature_names=dv.get_feature_names()))\n",
        "```"
      ],
      "metadata": {
        "id": "pVdWG9NOwcpX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.4 Decision Tree Learning Algorithm\n",
        "\n",
        "***Find the best split algorithm:***\n",
        "```\n",
        "for F in features:\n",
        "    find all thresholds for F\n",
        "    for T in thresholds:\n",
        "        split dataset using \"F > T\"\n",
        "        compute impurity of this split  (misclassification)\n",
        "select the condition with the lowest impurity\n",
        "```\n",
        "***Stopping criteria:***\n",
        "- group is already pure\n",
        "- tree reached depth limit\n",
        "- group too small to split\n",
        "\n",
        "***Decision tree learning algorithm:***\n",
        "- find the best split\n",
        "- stop if max depth reached\n",
        "- if left is sufficiently large and not pure:  \n",
        "     -> repeat for left\n",
        "- if right is sufficiently large and not pure:  \n",
        "     -> repeat for right"
      ],
      "metadata": {
        "id": "sTBNj0L758bf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.5 Decision Trees Parameter Tuning\n",
        "Two features, **max_depth** and **min_samples_leaf** have a greater importance than other parameters.   \n",
        "```\n",
        "scores = []\n",
        "for m in [4, 5, 6]:\n",
        "    print('depth: %s' % m)\n",
        "\n",
        "    for s in [1, 5, 10, 15, 20, 50, 100, 200]:\n",
        "        dt = DecisionTreeClassifier(max_depth=m, min_samples_leaf=s)\n",
        "        dt.fit(X_train, y_train)\n",
        "        y_pred = dt.predict_proba(X_val)[:, 1]\n",
        "        auc = roc_auc_score(y_val, y_pred)\n",
        "        scores.append((d, s, auc))\n",
        "```\n",
        "Ð dataframe is created with all possible combinations of *max_depth*, *min_sample_leaf* and the *auc score* corresponding to them.\n",
        "```\n",
        "df_scores = pd.DataFrame(scores, columns = [\"max_depth\", \"...\", \"...\"])\n",
        "df_scores_pivot = df_scores.pivot(index='min_samples_leaf', columns=['max_depth'], values=['auc'])\n",
        "``` \n",
        "These results are visualized using a **heatmap** by pivoting the dataframe to easily determine the best possible max_depth and min_samples_leaf combination. \n",
        "```\n",
        "sns.heatmap(df_scores_pivot, annot=True)\n",
        "```\n",
        "Finally, the DT is retrained using the identified parameter combination. DT so trained is viewed as a tree diagram."
      ],
      "metadata": {
        "id": "kDsvLIqB1ZEt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.6 Ensemble learning and random forest\n",
        "**Random Forest** is an example of **ensemble** learning where each model is a decision tree and their predictions are aggregated to identify the most popular result. Random forest only select a random subset of features from the original data to make predictions. In random forest the decision trees are trained independently.\n",
        "\n",
        "Tuning the *max_depth* parameter:\n",
        "```\n",
        "all_aucs = {}\n",
        "\n",
        "for depth in [5, 10, 20]:\n",
        "    print('depth: %s' % depth)\n",
        "    aucs = []\n",
        "\n",
        "    for i in range(10, 201, 10):\n",
        "        rf = RandomForestClassifier(n_estimators=i, max_depth=depth,random_state=1)\n",
        "        rf.fit(X_train, y_train)\n",
        "        y_pred = rf.predict_proba(X_val)[:, 1]\n",
        "        auc = roc_auc_score(y_val, y_pred)\n",
        "        print('%s -> %.3f' % (i, auc))\n",
        "        aucs.append(auc)\n",
        "    \n",
        "    all_aucs[depth] = aucs\n",
        "```\n",
        "Tuning the *min_samples_leaf* parameter:\n",
        "```\n",
        "all_aucs = {}\n",
        "\n",
        "for m in [3, 5, 10]:\n",
        "    print('min_samples_leaf: %s' % m)\n",
        "    aucs = []\n",
        "\n",
        "    for i in range(10, 201, 20):\n",
        "        rf = RandomForestClassifier(n_estimators=i, max_depth=10, min_samples_leaf=m, random_state=1)\n",
        "        rf.fit(X_train, y_train)\n",
        "        y_pred = rf.predict_proba(X_val)[:, 1]\n",
        "        auc = roc_auc_score(y_val, y_pred)\n",
        "        print('%s -> %.3f' % (i, auc))\n",
        "        aucs.append(auc)\n",
        "    \n",
        "    all_aucs[m] = aucs\n",
        "    print()\n",
        "```"
      ],
      "metadata": {
        "id": "FUKRugS1DhJh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.7 Gradient boosting and XGBoost\n",
        "Unlike Random Forest where each decision tree trains *independently*, in the **Gradient Boosting Trees**, the models are combined *sequentially* where each model takes the prediction errors made my the previous model and then tries to improve the prediction. This process continues to `n` number of iterations and in the end all the predictions get combined to make *final prediction*.  \n",
        "\n",
        "**XGBoost** is one of the libraries which implements the gradient boosting technique.\n",
        "```\n",
        "!pip install xgboost\n",
        "```\n",
        "- To train and evaluate the model, we need to wrap our train and validation data into a special data structure from XGBoost which is called **DMatrix**. This data structure is optimized to train xgboost models faster.  \n",
        "```\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train, feature_names=dv.feature_names_)\n",
        "dval = xgb.DMatrix(X_val, label=y_val, feature_names=dv.feature_names_)\n",
        "```\n",
        "- **xgb_params**: key-value pairs of hyperparameters to train xgboost model.\n",
        "```\n",
        "xgb_params = {\n",
        "    'eta': 0.3,\n",
        "    'max_depth': 6,\n",
        "    'min_child_weight': 1,\n",
        "\n",
        "    'objective': 'binary:logistic',\n",
        "    'nthread': 8,\n",
        "    'seed': 1\n",
        "}\n",
        "model = xgb.train(xgb_params, dtrain, num_boost_round=10)\n",
        "```\n",
        "- **watchlist**: list to store training and validation accuracy to evaluate the performance of the model after each training iteration. The list takes tuple of train and validation set from DMatrix wrapper:\n",
        "```\n",
        "watchlist = [(dtrain, 'train'), (dval, 'val')]\n",
        "```\n",
        "- **`%%capture output`**: IPython magic command which captures the standard output and standard error of a cell.\n",
        "- the last step is to **parse** xgb output and make a plot\n",
        "\n"
      ],
      "metadata": {
        "id": "Yhkz7XohLNSu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.8 XGBoost parameter tuning\n",
        "XGBoost has various tunable parameters but the three most important ones are:\n",
        "\n",
        "- **eta** (default=0.3)\n",
        " - It is also called learning_rate and is used to prevent overfitting by regularizing the weights of new features in each boosting step. range: [0, 1]\n",
        "\n",
        "- **max_depth** (default=6)\n",
        "  - Maximum depth of a tree. Increasing this value will make the model mroe complex and more likely to overfit. range: [0, inf]\n",
        "\n",
        "- **min_child_weight** (default=1)\n",
        "  - Minimum number of samples in leaf node. range: [0, inf] \n",
        "\n",
        "\n",
        "Sequence:\n",
        "\n",
        "1) find the best value for eta  \n",
        "2) find the best value for max_depth  \n",
        "3) find the best value for min_child_weight  \n",
        "\n",
        "\n",
        "___________\n",
        "\n",
        "Other useful parameter are:\n",
        "\n",
        "- subsample (default=1)\n",
        "  - Subsample ratio of the training instances. Setting it to 0.5 means that model would randomly sample half of the trianing data prior to growing trees. range: (0, 1]\n",
        "- colsample_bytree (default=1)\n",
        "  - This is similar to random forest, where each tree is made with the subset of randomly choosen features.\n",
        "- lambda (default=1)\n",
        "  - Also called reg_lambda. L2 regularization term on weights. Increasing this value will make model more conservative.\n",
        "- alpha (default=0)\n",
        "  - Also called reg_alpha. L1 regularization term on weights. Increasing this value will make model more conservative."
      ],
      "metadata": {
        "id": "z5iiYuJKSXcH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.9 Selecting the best model\n",
        "We select the final model from *decision tree, random forest, or xgboost* based on the best auc scores. After that we prepare the `df_full_train` and `df_test` to train and evaluate the final model. If there is not much difference between model auc scores on the train as well as test data then the model has generalized the patterns well enough.\n",
        "\n",
        "Generally, XGBoost models perform better on tabular data than other machine learning models but the downside is that these model are easy to overfit cause of the high number of hyperparameter. Therefore, XGBoost models require a lot more attention for parameters tuning to optimize them."
      ],
      "metadata": {
        "id": "pRnWcwD1V2DX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E2N1LgflWdKM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}