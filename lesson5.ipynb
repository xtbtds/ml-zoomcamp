{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMaE4G/kD64lasRxYmykDc0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xtbtds/ml-zoomcamp/blob/main/lesson5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.1 Intro\n",
        "\n",
        "***Deployment of the model*** - use the model to predict new values without running the code. The way to do this is to deploy the model in a server (run the code and make the model). After deploying the code in a machine used as server we can make some **endpoints** (using api's) to connect from another machine to the server and predict values.\n",
        "\n"
      ],
      "metadata": {
        "id": "zYQbhGktJtg-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.2 Saving and loading the model\n",
        "\n",
        "- To save the model we made before there is an option using the **pickle library**:\n",
        "  - First install the library with the command `pip install pickle-mixin` if you don't have it.\n",
        "  - After training the model and being the model ready for prediction process use this code to save the model for later.\n",
        "  ``` \n",
        "  import pickle\n",
        "with open('model.bin', 'wb') as f_out:\n",
        "   pickle.dump((dcit_vectorizer, model), f_out)\n",
        "f_out.close() ## After opening any file it's nessecery to close it\n",
        "  ```\n",
        "  - In the code above we'll making a **binary file named model.bin** and writing the dict_vectorizer for one hot encoding and model as array in it. (We will save it as binary in case it wouldn't be readable by humans)\n",
        "  - To be able to use the model in future without running the code, We need to open the binary file we saved before.\n",
        "  ```\n",
        "with open('mode.bin', 'rb') as f_in:  ## never open a binary file you do not trust!\n",
        "    dict_vectorizer, model = pickle.load(f_in)\n",
        "f_in.close()\n",
        "  ```\n",
        "  - With unpacking the model and the dict_vectorizer, We're able to again predict for new input values without training a new model by re-running the code."
      ],
      "metadata": {
        "id": "pUDu11Z6NIVe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.3 Web services: introduction to Flask\n",
        "\n",
        "A web service is a method used to communicate between electronic devices.\n",
        "\n",
        "- Methods:\n",
        "  - **GET**: a method used to retrieve files, for example when we are searching for a cat image in google we are actually requesting cat images with GET method.\n",
        "  - **POST**: for example in a sign up process, when we are submiting our name, username, passwords, etc we are posting our data to a server that is using the web service (there is no specification where the data goes).\n",
        "  - **PUT**: same as POST but we are specifying where the data is going to.\n",
        "  - **DELETE**: a method that is used to request to delete some data from the server.\n",
        "\n",
        "- 0.0.0.0 vs localhost\n",
        "  - process that is listening on 127.0.0.1 for connections will only receive *local* connections on that socket.\n",
        "  - server is told to listen on 0.0.0.0 will \"listen on every available network interface\"\n",
        "\n",
        "- Create Flask web service\n",
        "  - `pip install Flask`\n",
        "  -\n",
        "  ```\n",
        "  from flask import Flask\n",
        "app = Flask('churn-app')\n",
        "@app.route('/ping',methods=[GET])\n",
        "def ping():\n",
        "      return 'PONG'\n",
        "if __name__=='__main__':\n",
        "      app.run('debug=True, host='0.0.0.0', port=9696)\n",
        "  ```\n",
        "  - open your browser and search localhost:9696/ping or type `curl 0.0.0.0:9696/ping`"
      ],
      "metadata": {
        "id": "o1urZlNZNx27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.4 Serving the churn model with Flask\n",
        "\n",
        "- First, load the previous saved model and use a prediction function in a special route.\n",
        "```\n",
        "import pickle\n",
        "with open('churn-model.bin', 'rb') as f_in:\n",
        "      dv, model = pickle.load(f_in)\n",
        "  ```\n",
        "- Predict a value for a customer\n",
        "```\n",
        "def predict_single(customer, dv, model):\n",
        "  X = dv.transform([customer])  ## apply the one-hot encoding feature to the customer data \n",
        "  y_pred = model.predict_proba(X)[:, 1]\n",
        "  return y_pred[0]\n",
        "```\n",
        "- Make the final function used for creating the web service.\n",
        "```\n",
        "@app.route('/predict', methods=['POST'])  \n",
        "## send the customer information = POST\n",
        "def predict():\n",
        "    customer = request.get_json()  ## access the body of json.\n",
        "    prediction = predict_single(customer, dv, model)\n",
        "    churn = prediction >= 0.5\n",
        "    result = {\n",
        "        'churn_probability': float(prediction), ## conver numpy data into python data in flask framework\n",
        "        'churn': bool(churn),  ## conver\n",
        "    }\n",
        "    return jsonify(result)  ## send back the data in json format to the user\n",
        "```\n",
        "- To see the result can't use a simple request in web browser. We can run the code below to post a new user data and see the response. Browser works corretly with GET requests, but we have POST.\n",
        "```\n",
        "## a new customer informations\n",
        "customer = {\n",
        "  'customerid': '8879-zkjof',\n",
        "  'gender': 'female',\n",
        "  'seniorcitizen': 0,\n",
        "  'partner': 'no',\n",
        "  'dependents': 'no',\n",
        "  'tenure': 41,\n",
        "  'phoneservice': 'yes',\n",
        "  'multiplelines': 'no',\n",
        "  'internetservice': 'dsl',\n",
        "  'onlinesecurity': 'yes',\n",
        "  'onlinebackup': 'no',\n",
        "  'deviceprotection': 'yes',\n",
        "  'techsupport': 'yes',\n",
        "  'streamingtv': 'yes',\n",
        "  'streamingmovies': 'yes',\n",
        "  'contract': 'one_year',\n",
        "  'paperlessbilling': 'yes',\n",
        "  'paymentmethod': 'bank_transfer_(automatic)',\n",
        "  'monthlycharges': 79.85,\n",
        "  'totalcharges': 3320.75\n",
        "}\n",
        "```\n",
        "```\n",
        "import requests ## to use the POST method \n",
        "url = 'http://localhost:9696/predict' ## the route for prediction\n",
        "response = requests.post(url, json=customer) ## post the customer information in json format\n",
        "result = response.json() ## get the server response\n",
        "print(result)\n",
        "```\n",
        "-  When you run your app you will see a warning that it is not a **WGSI server and not suitable for production environmnets**.\n",
        "  - **gunicorn** - for Linux and Mac OS\n",
        "\n",
        "    - install: `pip install gunicorn`\n",
        "    - run: `gunicorn --bind 0.0.0.0:9696 churn:app`\n",
        "    - in churn:app the name churn is the name we set for our the file containing the code app = Flask('churn')(for example: churn.py)\n",
        "  - **waitress** - for Windows\n",
        "    - install: `pip install waitress`\n",
        "    - run: `waitress-serve --listen=0.0.0.0:9696 churn:app`\n",
        "\n"
      ],
      "metadata": {
        "id": "1JdNutcJc50v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.5 Python virtual environment: Pipenv\n",
        "\n",
        "- Every time we're running a file from a directory we're using the executive files from a global directory. For example when we install python on our machine the executable files that are able to run our codes will go to somewhere like /home/username/python/bin/ for example the pip command may go to /home/username/python/bin/pip.\n",
        "- Sometimes the versions of libraries conflict (the project may not run or get into massive errors). For example we have an old project that uses sklearn library with the version of 0.24.1 and now we want to run it using sklearn version 1.0.0. We may get into errors because of the version conflict.\n",
        "- To solve the conflict we can make virtual environments. Virtual environment is something that can seperate the libraries installed in our system and the libraries with specified version we want our project to run with. \n",
        "- **pipenv**\n",
        "  - install: `pip install pipenv`\n",
        "  - install the libraries we want for our project in the new virtual environment: `pipenv install numpy sklearn==0.24.1 flask`\n",
        "  - using the pipenv command we made two files Pipfile and Pipfile.lock\n",
        "  - If we want to run the project in another machine, we run `pipenv install`. This command will look into Pipfile and Pipfile.lock to install the libraries with specified version.\n",
        "  - run the project in the virtual environment: `pipenv shell`"
      ],
      "metadata": {
        "id": "lAx8rOCRnmTH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.6 Environment management: Docker\n",
        "\n",
        "To isolate more our project file from our system machine. With Docker you are able to pack all your project is a system that you want and run it in any system machine.   \n",
        "- **Docker**\n",
        "  - `sudo apt-get install docker.io`\n",
        "- Once our project was packed in a Docker container, we're able to run our project on any machine.\n",
        "- First we have to make a Docker image. In Docker image file there are settings and dependecies we have in our project. To find Docker images that you need you can simply search the Docker website.\n",
        "- Create **Docker file**\n",
        "    ```\n",
        "    # First install the python 3.8, the slim version have less size\n",
        "    FROM python:3.8.12-slim\n",
        "    ```\n",
        "    ```\n",
        "    # Install pipenv library in Docker \n",
        "    RUN pip install pipenv\n",
        "    ```\n",
        "    ```\n",
        "    # we have created a directory in Docker named app and we're using it as work directory \n",
        "    WORKDIR /app    \n",
        "    ```\n",
        "    ```                                                            \n",
        "    # Copy the Pip files into our working derectory \n",
        "    COPY [\"Pipfile\", \"Pipfile.lock\", \"./\"]\n",
        "    ```\n",
        "    ```\n",
        "    # install the pipenv dependecies we had from the project and deploy them \n",
        "    RUN pipenv install --deploy --system\n",
        "    ```\n",
        "    ```\n",
        "    # Copy any python files and the model we had to the working directory of Docker \n",
        "    COPY [\"*.py\", \"churn-model.bin\", \"./\"]\n",
        "    ```\n",
        "    ```\n",
        "    # We need to expose the 9696 port because we're not able to communicate with Docker outside it\n",
        "    EXPOSE 9696\n",
        "    ```\n",
        "    ```\n",
        "    # If we run the Docker image, we want our churn app to be running\n",
        "    ENTRYPOINT [\"gunicorn\", \"--bind\", \"0.0.0.0:9696\", \"churn_serving:app\"]\n",
        "    ```\n",
        "    - If we don't put the last line ENTRYPOINT, we will be in a python shell. Note that for the entrypoint, we put our commands in doble quotes.\n",
        "- Build:\n",
        "`docker build -t churn-prediction`\n",
        "- Run: `docker run -it -p 9696:9696 churn-prediction:latest`\n",
        "  - first 9696 is the port number of our machine and the last one is Docker container port"
      ],
      "metadata": {
        "id": "ANLuqd-cBD6j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vJDVARmia6uD"
      }
    }
  ]
}