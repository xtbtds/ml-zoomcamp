{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 8.2 Tensorflow and Keras\n",
        "There is a special function in keras to load an image:\n",
        "```\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "img = load_img(fullname, target_size=(299, 299))\n",
        "x = np.array(img)\n",
        "```\n",
        "\n",
        "Image has 3 channels (RGB) each of which is a Numpy array, and this array contains numbers from 0 to 255."
      ],
      "metadata": {
        "id": "MeWmJYvNuHF8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.3 Pre-trained CNN\n",
        "There many pretrained models for Keras: https://keras.io/api/applications/\n",
        "\n",
        " To use one of them, *Xception*, run:\n",
        "```\n",
        "from tensorflow.keras.applications.xception import Xception\n",
        "model = Xception(weights='imagenet', input_shape=(299, 299, 3))\n",
        "X = np.array([x])\n",
        "pred = model.predict(X)\n",
        "```\n",
        "\n",
        "The results don't make any sense, because our data needs *preprocessing*:\n",
        "```\n",
        "X = preprocess_input(X)\n",
        "pred = model.predict(X)\n",
        "```\n",
        "The values [0; 255] are transformed to [-1; 1].\n",
        "\n",
        "To make the predictions human-readable:\n",
        "```\n",
        "from tensorflow.keras.applications.xception import decode_predictions\n",
        "decode_predictions(pred)\n",
        "```\n",
        "Here we see that for our image models predicts \"jersey\". It looks like a T-shirt, but remember that we use a pretrained model \"*imagenet*\" - it doesn't have class \"T-shirt\", the most closest class it has is \"jersey\"."
      ],
      "metadata": {
        "id": "avRrgmEpavRD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.4 Convolutional neural networks\n",
        "\n",
        "CNN consists of: ***Convolutional layers, Vector representation, Dense layers***.\n",
        "\n",
        "### 1. Convolutional layers  \n",
        "Each layer consists of ***filters*** - small images (5x5, 3x3, etc), that contain simple shapes:   \n",
        "- layer 1 - the simplest shapes: $-$, $/$, $|$, c\n",
        "- layer 2 - more complex shapes: x, o, T\n",
        "- layer 3 - even more complex shapes: s, w, q, [, ]\n",
        "- ...\n",
        "\n",
        "Inside the ***first*** layer each filter is thrown through the image building a ***feature map*** - 2x2 array with numbers. The bigger is number, the more similar is this part of image to the current filter. \n",
        "The output of each layer are these feature maps, their number is equal to the number of filters in the current layer - ***one feature map per filter***.    \n",
        "Inside the ***second*** layer, there are built new feature maps, using feature maps from the previous layer and new filters - so here more complex forms can be detected.  \n",
        "For example: specific piece of image has big similarity with \\ and /, so first feature map has big numbers in this place. Second layer will probably recognise here a cross - X.\n",
        "\n",
        "### 2. Vector representation\n",
        "After all CLs, ***1-dim vector of features*** is built. It can have different areas for different types of features, referring to, for example, sleeve recognition, straight lines recognition etc - all info about image that NN was able to extract. The length of the array is equal to multiplied parameters of image - 299x299x3 (if there was no ***pooling*** of course).\n",
        "\n",
        "### 3. Dense layers.\n",
        "***Fully-connected layers.***\n",
        "- *Binary classification*:  \n",
        "  - $x = [x_1, x_2, x_3, ..., x_n] - $ feature array.   \n",
        "  - $∑x_iw_i ⇒$ sigmoid $⇒$ predictions.  \n",
        "\n",
        "- *Multiclass classification*:  \n",
        "  - Using different trained weights for each class\n",
        "  - $∑x_iw^1_i, ∑x_iw^2_i, ∑x_iw^3_i, ... ∑x_iw^m_i ⇒$ softmax $⇒$ $m$-dim array for probabilities of each of $m$ classes.\n",
        "\n",
        "\n",
        "### 4. Pooling (CLs)\n"
      ],
      "metadata": {
        "id": "X3aGtMtUsow6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.5 Transfer learning\n",
        "\n",
        "***Transfer learning*** (TL) - focuses on storing knowledge gained while solving one problem and applying it to a different but related problem. We will take pretrained CNN and vector representation from Xception \"imagenet\". We won't use a dense layer, because its specific to \"imagenet\": 1000 classes predicting different types of things, when we need 10 custom classes to predict only types of clothes we have.\n",
        "### Loading data\n",
        "```\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "train_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "train_ds = train_gen.flow_from_directory(\n",
        "    './clothing-dataset-small/train',\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32\n",
        ")\n",
        "train_ds.class_indices\n",
        "X, y = next(train_ds)\n",
        "y[:5]\n",
        "```\n",
        "The same for validation dataset, but `shuffle=False`.\n",
        "### Model building\n",
        "We keep CNN, it's our ***base model***. We don't train it, just \"freeze\" and use. Only dense layers will be trained.\n",
        "\n",
        "- `include_top=False` means we don't include dense layers from \"imagenet\"; by the way, \"bottom\" is CNN and vector representation. Remember we ***don't train*** this part:\n",
        "```\n",
        "base_model = Xception(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_shape=(150, 150, 3)\n",
        ")\n",
        "base_model.trainable = False   \n",
        "```\n",
        "\n",
        "- Creating a new \"top\":\n",
        "```\n",
        "inputs = keras.Input(shape=(150, 150, 3))\n",
        "```\n",
        "\n",
        "- Then we build the rest of our model:\n",
        "```\n",
        "base = base_model(inputs, training=False)\n",
        "```\n",
        "\n",
        "- The shape of outputs from base_model is (32, 5, 5, 2048). To use dense layers we need 2-dim array, that's why we need to apply ***pooling***. It takes average from each 5x5 image:\n",
        "```\n",
        "vectors = keras.layers.GlobalAveragePooling2D()(base)\n",
        "```\n",
        "- Above and under we use functions as it looks like in functional programming.\n",
        "- Dense layer with the output of 10 classes:\n",
        "```\n",
        "outputs = keras.layers.Dense(10)(vectors)\n",
        "model = keras.Model(inputs, outputs)\n",
        "```\n",
        "- Next predictions doesn't make any sense, because we have only build a model, but haven't trained it yet:\n",
        "```\n",
        "preds = model.predict(X)\n",
        "preds[0]\n",
        "```\n",
        "\n",
        "## Training model and optimizer\n",
        "- \n",
        "```\n",
        "learning_rate = 0.01\n",
        "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "```\n",
        "- The parameter of dense layer is matrix of weights $W$. Optimizer will change the weights, and looking into losses, model will find the best $W$. `from_logits=True` means we don't apply $softmax$. There will be raw values in the output instead of probabilities.\n",
        "```\n",
        "loss = keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "history = model.fit(train_ds, epochs=10, validation_data=val_ds)\n",
        "history.history['accuracy']\n",
        "```\n"
      ],
      "metadata": {
        "id": "jXxfN_he3148"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.6 Adjusting the learning rate\n",
        "\\begin{array}{|c|c|c|c|} \\hline\n",
        "\\text{ learning_rate}& \\text{speed} & \\text{apply on val data} & \\text{explanation} & \\text{overfitting}\\\\ \\hline\n",
        "\\text{high} & \\text{fast} & \\text{poorly} & \\text{many books/year, bad applying} & \\text{yes}\\\\\n",
        "\\text{medium} & \\text{OK} & \\text{good} & \\text{norm books/year, norm applying} & \\text{OK}\\\\ \n",
        "\\text{low} & \\text{slooooow} & \\text{very well} & \\text{little books/year, great applying} & \\text{no}\\\\  \\hline\n",
        "\\end{array}\n",
        "\n",
        "- Make a separate function for model building:\n",
        "```\n",
        "def make_model(learning_rate=0.01):\n",
        "    base_model = Xception(\n",
        "        ...\n",
        "    )\n",
        "\n",
        "    base_model.trainable = False\n",
        "#########################################\n",
        "    inputs = keras.Input(shape=(150, 150, 3))\n",
        "    base = base_model(inputs, training=False)\n",
        "    vectors = keras.layers.GlobalAveragePooling2D()(base)\n",
        "    outputs = keras.layers.Dense(10)(vectors)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "#########################################\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    loss = keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "    return model\n",
        "  ```\n",
        "\n",
        "- Adjusting learning rate:\n",
        "```\n",
        "scores = {}\n",
        "for lr in [0.0001, 0.001, 0.01, 0.1]:\n",
        "    print(lr)\n",
        "    model = make_model(learning_rate=lr)\n",
        "    history = model.fit(train_ds, epochs=10, validation_data=val_ds)\n",
        "    scores[lr] = history.history\n",
        "```"
      ],
      "metadata": {
        "id": "1ilg8JCHw_pW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.7 Checkpointing\n",
        "- We want to save the model after each training epoch. Particularely, save only the best models.\n",
        "- Save our old model:\n",
        "```\n",
        "model.save_weights('model_v1.h5', save_format='h5')\n",
        "```\n",
        "-\n",
        "```\n",
        "chechpoint = keras.callbacks.ModelCheckpoint(\n",
        "    'xception_v1_{epoch:02d}_{val_accuracy:.3f}.h5',\n",
        "    save_best_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max'       #maximize accuracy\n",
        ")\n",
        "learning_rate = 0.001\n",
        "model = make_model(learning_rate=learning_rate)\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    epochs=10,\n",
        "    validation_data=val_ds,\n",
        "    callbacks=[chechpoint]\n",
        ")\n",
        "```\n"
      ],
      "metadata": {
        "id": "JKs1yEd11bDS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.8 Adding More Layers\n",
        "Inner dense layers must have ***activation functions***. For output we have already used softmax and sigmoid. One of the popular for the inner layers is ***ReLU***:\n",
        "\\begin{equation*}\n",
        "ReLU(x) = \n",
        " \\begin{cases}\n",
        "   0 &\\text{if } x < 0,\\\\\n",
        "   x &\\text{if } x \\geq 0\n",
        " \\end{cases}\n",
        "\\end{equation*}\n",
        "\n",
        "- Edit function `make_model`:\n",
        "```\n",
        "def make_model(learning_rate=0.01, size_inner=100):\n",
        "    ...\n",
        "    inner = keras.layers.Dense(size_inner, activation='relu')(vectors)\n",
        "    outputs = keras.layers.Dense(10)(inner)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "```\n",
        "- Adjust the output size of inner layer:\n",
        "```\n",
        "learning_rate = 0.001\n",
        "scores = {}\n",
        "for size in [10, 100, 1000]:\n",
        "    print(size)\n",
        "\n",
        "    model = make_model(learning_rate=learning_rate, size_inner=size)\n",
        "    history = model.fit(train_ds, epochs=10, validation_data=val_ds)\n",
        "    scores[size] = history.history\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "QhVp07ELSV91"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.9 Regularization and Dropout\n",
        "- A neural network might ***learn false patterns***, i.e. if it repeatedly recognizes a certain logo on a t-shirt it might learn that the logo defines the t-shirt which is wrong since the logo might also be seen on a hoodie. Model needs to focus on ***overall shape, not on details*** like logos.\n",
        "- Main idea is hiding parts of the images (freeze) from being seen by the learning neural network.\n",
        "- ***Dropout*** - randomly freezing parts of the image. We won't update some neurons in the inner layer each steps.\n",
        "- Lets modify function `make_model`:\n",
        "```\n",
        "def make_model(learning_rate=0.01, size_inner=100, droprate=0.5):\n",
        "    ...\n",
        "```  \n",
        "  ***Droprate*** - how much of neurons we freeze.\n",
        "```\n",
        "    inner = keras.layers.Dense(size_inner, activation='relu')(vectors)\n",
        "    drop = keras.layers.Dropout(droprate)(inner)\n",
        "    outputs = keras.layers.Dense(10)(drop)\n",
        "```\n",
        "- Adjust the droprate, increase the number of epochs:\n",
        "```\n",
        "learning_rate = 0.001\n",
        "size = 100\n",
        "scores = {}\n",
        "for droprate in [0.0, 0.2, 0.5, 0.8]:\n",
        "    print(droprate)\n",
        "    model = make_model(\n",
        "        learning_rate=learning_rate,\n",
        "        size_inner=size,\n",
        "        droprate=droprate\n",
        "    )\n",
        "    history = model.fit(train_ds, epochs=30, validation_data=val_ds)\n",
        "    scores[droprate] = history.history\n",
        "```\n",
        "- Analyze graphics. Remember there can be *overfitting*: when accuracy on train is much more than accuracy on val.\n",
        "```\n",
        "for droprate, hist in scores.items():\n",
        "    plt.plot(hist['val_accuracy'], label=('val=%s' % droprate))\n",
        "plt.ylim(0.78, 0.86)\n",
        "plt.legend()\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "CFfEOHouVxTt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.10 Data Augmentation\n",
        "***Data augmentation*** - generating more images from existing ones.\n",
        "- Possible image transformations:\n",
        "  - flip (horizontally, vertically)\n",
        "  - rotation\n",
        "  - shift (top, bottom, left, right)\n",
        "  - shear\n",
        "  - shrink (x, y)\n",
        "  - zoom (in/out)\n",
        "  - brightness/contrast\n",
        "  - black patch\n",
        "  - combine all\n",
        "- Choosing augmentations\n",
        "  - Use your own judgement\n",
        "  - Look at the dataset. What kind of variations are there?\n",
        "    - Are the objects always centered? (Rotate, shift)\n",
        "  - Tune it as a hyperparameter.  \n",
        "    Train it for 10-20 epochs. Is it better?\n",
        "      - Yes ⇒ use\n",
        "      - No ⇒ don't use\n",
        "      - Same ⇒ train for more epochs\n",
        "\n",
        "***Examples***: https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/chapter-07-neural-nets/07-augmentations.ipynb\n"
      ],
      "metadata": {
        "id": "c-_1ce7me94C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.11 Training a Larger Model\n"
      ],
      "metadata": {
        "id": "bTIcVSGsoWQo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.12 Using the Model\n",
        "https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/08-deep-learning/notebook.ipynb"
      ],
      "metadata": {
        "id": "ApuSJpAVHZtc"
      }
    }
  ]
}